# deepdaiv_captioning
<img width="945" alt="Screenshot 2023-04-06 at 8 41 11 PM" src="https://github.com/rachel618/deepdaiv_captioning/assets/67910258/05b2cab3-b680-42c2-a199-ea897c159cd5">

## 1. Image Captioning

First, we generated captions for multiple images. For each photo, we generated 3 captions, and combined the captions of all uploaded photos into a single document.

### Multi-Input

There was a lot of deliberation about how to handle multiple photos—whether to process each photo individually and use the captions separately, or combine multiple images into a single input of the same size and perform inference only once, etc.

However, when we combined multiple images and input them into the model, the generated caption was quite general. For example, in the case of multiple beach photos, specific details such as people lying on the beach, cruises, or snorkeling were omitted, and captions like "a day at the beach" were generated. We found it difficult to obtain valid information to generate hashtags this way, so we decided to perform inference separately for each photo.

We also tried different methods, like combining photos vertically or horizontally, but the captions tended to focus on specific parts of the images rather than distributing attention evenly across all the photos. Even after increasing the number of captions, there were almost no captions for the last image. The accuracy was higher when each image was processed individually.

### Versatile Diffusion (VD)
<img width="821" alt="Screenshot 2023-04-06 at 9 14 42 PM" src="https://github.com/rachel618/deepdaiv_captioning/assets/67910258/ee476a64-b54b-4546-9e36-b70cfbcdfb37">

Demo: [https://huggingface.co/spaces/shi-labs/Versatile-Diffusion](https://huggingface.co/spaces/shi-labs/Versatile-Diffusion)

For the Image Captioning model, we chose Versatile Diffusion due to its decent accuracy and short inference time, considering the demo. While there are many models, like BLIP and OFA, Versatile Diffusion stood out for this purpose.

VD is an integrated multi-flow multimodal diffusion framework. Beyond image-to-text, it supports tasks like text-to-image, image transformation, and editable I2T2I (image-to-text-to-image). Editable I2T2I allows you to modify an image based on changes made to the caption generated by the model.

VD integrates various flows as inputs and simultaneously learns them. The core of its diffusion network is grouping and exchanging networks based on input and output data types. 
<img width="411" alt="Screenshot 2023-04-06 at 10 01 30 PM" src="https://github.com/rachel618/deepdaiv_captioning/assets/67910258/c2407c63-fcce-4c30-a2b6-c48bb8a632fa">

As shown above, all diffuser layers are classified into global layers, data layers, and context layers.

From the layer names, we can guess their functions. The global layer is shared across all tasks, while the context layer and data layer are used when corresponding input and output types are entered. For example, for image-to-text tasks, the diffuser uses the image context layer and the text data layer.

Since some layers are shared across tasks, VD seems to understand semantic information better than single-flow models (like Latent Diffusion or Stable Diffusion, which only do text-to-image).

## 2. Extracting Keywords from Captions

### YAKE

[YAKE!](https://www.notion.so/YAKE-dabe536b203b4b5893b77b43f3f51ad7?pvs=21)

Simply put, YAKE is a statistical model that calculates the importance of words in a document using five indicators. While it has many strengths, YAKE relies on techniques used in writing. It assumes that terms in uppercase are more important than lowercase terms, and that words that appear at the beginning of a document are more significant than those appearing later.

However, the first photo’s caption may not always be the most important, and since captions don’t typically use capital letters for emphasis, YAKE isn’t suitable for extracting keywords from documents combining multiple captions. Instead, we used DistilBERT.

### DistilBERT

BERT is a well-known and influential model among the "Large Language Models," trained on vast amounts of web documents. To understand BERT’s learning process, imagine filling in the blanks of a sentence in a foreign language—BERT learns natural language in a similar way.

BERT is versatile and performs well across various tasks after Fine Tuning (task-specific additional training). DistilBERT, a distilled version of BERT, was developed to be lighter and more efficient.

Distillation in deep learning refers to transferring the knowledge learned by a large model to a smaller one. You can find more details about Knowledge Distillation [here](https://cpm0722.github.io/paper-review/distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter).

Using the ‘transformers’ library from Hugging Face, we easily loaded the DistilBERT model via the SentenceTransformer module. Below is the code for keyword extraction using DistilBERT:

```python
# keyword extraction setting
n_gram_range = (1, 1)
stop_words = "english"
count = CountVectorizer(ngram_range=n_gram_range,
                        stop_words=stop_words).fit([docs])
candidates = count.get_feature_names_out()  # list

# keyword extraction model
ke_model = SentenceTransformer(
    'sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1')
doc_embedding = ke_model.encode([docs])
candidate_embeddings = ke_model.encode(candidates)

distances = cosine_similarity(doc_embedding, candidate_embeddings)
keywords = [candidates[index]
            for index in distances.argsort()[0][-num_of_cores:]]  # list
```

## 3. Recommending Relevant Tags

### Word2Vec

We used the Word2Vec model to supplement the semantic information missed by the captioning model. Initially, we aimed to train it on daily-life data, but many keywords extracted from the captions were not in Word2Vec's training data. As a result, it couldn't compute word similarity, preventing relevant hashtag recommendations.

Due to the large vocab size used by diffusion models like GPT and Optimus, we resorted to using a pre-trained 758MB model based on Twitter data (2 billion tweets, 27 billion tokens, 1.2 million uncased vocab).

## 4. Impression Tags

When planning our Instagram hashtag recommendation service, we considered user demand. We weren’t sure if users preferred fewer, highly accurate hashtags or a large number of suggestions, or if they just wanted hashtags optimized for exposure. We decided to let users choose from all options.

For impression hashtags, we referred to the 2023 best hashtags for Instagram virality. These hashtags, like #fashion, #pet, #Reels, are frequently used and thus advantageous for exposure.

While we could group recommendations based on image information for more tailored suggestions, we opted for user-selected checkboxes to enhance inference time and convenience. Users can choose from 14 categories.

| Category | Hashtags |
| --- | --- |
| like | '#likes', '#like', '#follow', ... |
| fashion | '#fashion', '#style', '#beauty', ... |
| food | '#food', '#foodie', '#yummy', ... |
| pet | '#pet', '#dog', '#cat', ... |
| tech | '#tech', '#gadgets', '#innovation', ... |
| wedding | '#wedding', '#bride', '#groom', ... |
| fitness | '#fitness', '#gym', '#workout', ... |
| travel | '#travel', '#nature', '#wanderlust', ... |
| holiday | '#christmas', '#halloween', '#newyear', ... |
| photo | '#photography', '#picoftheday', '#portrait', ... |
| music | '#music', '#hiphop', '#guitar', ... |
| art | '#art', '#drawing', '#artist', ... |
| nature | '#nature', '#flowers', '#wildlife', ... |
| Reels | '#reels', '#instareels', '#foryoupage', ... |

# 3. Demo

We used Gradio for the demo. Gradio allows for simple web interfaces for machine learning models. For instance, in an image classification model, you can input an image and get a text output (like "cat" or "dog"). Gradio creates a web page with this functionality when paired with UI design code.

Due to the large pre-trained model, we used the paid version of Google Colab to manage high RAM usage.
 <img width="1608" alt="Untitled" src="https://github.com/rachel618/deepdaiv_captioning/assets/67910258/fb13fdf7-2821-4ac8-83b6-2543fd8c10e3">

The demo interface works as follows:

1. Use the sliding bar to adjust the number of input photos (minimum 1, maximum 10).
2. Use the checkboxes to select the types of hashtags you want.
3. Drag and drop files or search for images online to upload them.
4. Click submit, and hashtags will be generated in the textbox below.
5. If no checkboxes are selected, only the "most relevant hashtags" section will display tags.
6. After accepting the hashtags, they will be copied to your clipboard.
